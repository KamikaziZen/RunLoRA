{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b9b046-1b0f-4425-8ae6-d72a1bea7ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset \n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import AutoImageProcessor\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from runlora.modeling import RunLoRAModel\n",
    "from runlora import RunLoRACollection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.benchmark as benchmark\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from functools import partial\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, filemode='w', format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228b3639-1911-410c-a85b-4d69e85bf90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e7c67-4ef6-4666-a88a-b4cabe5e00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vit.modeling_vit import ViTSelfAttention, ViTSelfOutput, ViTIntermediate, ViTOutput, ViTEmbeddings\n",
    "from torch.nn import Linear\n",
    "from runlora.modeling import RunLoRALinear\n",
    "\n",
    "def report_hook(idx, module, input, output):\n",
    "    if isinstance(input, tuple):\n",
    "        print(idx, input[0].shape)\n",
    "        print(input[0].dtype)\n",
    "    else:\n",
    "        print(idx, input.shape)\n",
    "        print(input.dtype)\n",
    "    if isinstance(output, tuple):\n",
    "        print(idx, output[0].shape)\n",
    "        print(output[0].dtype)\n",
    "    else:\n",
    "        print(idx, output.shape)\n",
    "        print(output.dtype)\n",
    "    print()\n",
    "\n",
    "def hook_model(model, hook_func, target_classes):\n",
    "\n",
    "    handles = []\n",
    "    j = 0\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, target_classes):\n",
    "        # if isinstance(module, (ViTEmbeddings)):\n",
    "            handle = module.register_forward_hook(partial(hook_func, j))\n",
    "            handles.append(handle)\n",
    "            j+=1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _ = model(**input_batch)\n",
    "    \n",
    "    for handle in handles:\n",
    "        handle.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477c917-b4c5-44ff-b4dd-85a317f5a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_run_time = 40\n",
    "type_string = 'fp32'\n",
    "dtype = torch.float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11754ae-c74a-48ab-972b-2a145da8959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_memory(reset_stats=True):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if reset_stats:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def bench_model(model, input_batch, min_run_time):\n",
    "    reset_memory()\n",
    "\n",
    "    bench = benchmark.Timer(\n",
    "        stmt='model(**input_batch).loss.backward()',\n",
    "        globals={'input_batch': input_batch, 'model': model})\n",
    "\n",
    "    # warmup\n",
    "    warmup_measure = bench.blocked_autorange(min_run_time=min_run_time)\n",
    "    assert len(warmup_measure.times) >= 10, \\\n",
    "        'Number of measurements is less than 10, increase min_run_time!'\n",
    "    \n",
    "    reset_memory()\n",
    "    max_mem_prev = torch.cuda.max_memory_allocated()\n",
    "    max_res_prev = torch.cuda.max_memory_reserved()\n",
    "\n",
    "    # benchmarking\n",
    "    measure = bench.blocked_autorange(min_run_time=min_run_time)\n",
    "    logging.info(\"Computing mean with {} measurments, {} runs per measurment\".format(\n",
    "        len(measure.times), measure.number_per_run))\n",
    "\n",
    "    max_mem = torch.cuda.max_memory_allocated()\n",
    "    max_res = torch.cuda.max_memory_reserved()\n",
    "\n",
    "    del bench\n",
    "    reset_memory()\n",
    "\n",
    "    logging.info(\"Mean time: {} us\".format(measure.mean * 1000000))\n",
    "    logging.info(\"Max Allocated Overhead: {} MB\".format((max_mem - max_mem_prev) / 2**20))\n",
    "    logging.info(\"Max Reserved Overhead:{} MB\".format((max_res - max_res_prev) / 2**20))\n",
    "    logging.info(\"\")\n",
    "\n",
    "    return {'mean_time_us': measure.mean * 1000000,\n",
    "            'max_mem_overhead_MB': (max_mem - max_mem_prev) / 2**20,\n",
    "            'max_mem_res_overhead_MB': (max_res - max_res_prev) / 2**20,\n",
    "            'msrs/runs': f'{len(measure.times)}/{measure.number_per_run}',\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51781395-b94e-42ce-8a08-2ffe37df7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_params(model):\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Params: {params}, Trainable Params: {trainable_params}')\n",
    "    return params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617866b-b9d2-45b8-bed4-63bdb93a0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"food101\", split=\"validation[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3803bc-16c9-44b3-9e55-3a99a5a28511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66bccb8-4df7-41a6-bf5c-ab74f06ef255",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]['image'].resize((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63704bb0-b630-4453-afca-93a9281c6644",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "\n",
    "id2label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f83f1-4d22-428b-8492-baa7c6c43881",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b920d33-5a11-4bbe-a5bd-e0cb04360659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the compute_metrics function takes a Named Tuple as input:\n",
    "# predictions, which are the logits of the model as Numpy arrays,\n",
    "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb78d6-368f-4255-bd56-0e71e0426980",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "if \"height\" in image_processor.size:\n",
    "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "    crop_size = size\n",
    "    max_size = None\n",
    "elif \"shortest_edge\" in image_processor.size:\n",
    "    size = image_processor.size[\"shortest_edge\"]\n",
    "    crop_size = (size, size)\n",
    "    max_size = image_processor.size.get(\"longest_edge\")\n",
    "\n",
    "val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(crop_size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97ebba-6a8d-4d7f-9c56-0fee1d529b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e0fa8-f2c8-448d-bb44-2bb026b6d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "labels = torch.tensor(dataset[:batch_size]['label'])\n",
    "# pixel_values = torch.stack(dataset[:batch_size]['pixel_values']).cuda()\n",
    "pixel_values = torch.stack(dataset[:batch_size]['pixel_values']).to(dtype).cuda()\n",
    "input_batch = {'pixel_values': pixel_values, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01700443-8d49-4aa8-8b71-5570ba293b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dfac5c-ccc8-4d52-958e-7cc632f48063",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_memory()\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "model = model.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131059e-43d5-48e8-a5a3-e04842242121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef91615-42b1-4215-a5d3-999c3ccff6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_classes = (ViTEmbeddings, ViTSelfAttention, ViTSelfOutput, ViTIntermediate, ViTOutput)\n",
    "target_classes = (Linear)\n",
    "hook_model(model, report_hook, target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0e07e-eeb9-4be1-a3dc-112b6e95a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = report_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dcdcb2-f953-480c-bf28-3e98b1dd6ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "reset_memory()\n",
    "stats = bench_model(model, input_batch, min_run_time=min_run_time)\n",
    "rows.append(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2def5-a2f0-423e-a564-e396a4a98131",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2715ba-ec35-426c-a4d5-50d2c773134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f24415-967c-4e44-bc6e-50aa88ec91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Allocated GPU Memory: {torch.cuda.memory_allocated() / 2**20}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270647bb-de34-4f2c-a44c-36420f55d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d65a6a-f121-4d7d-9b01-121d5cb8faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "lora_r = 32\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.\n",
    "target_modules = ['query', 'key', 'value', 'dense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fe3b6-8ab2-4ef7-b2b9-0a6a4113d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 768 * 768 > 2 * 768 * lora_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd4751-524a-4774-9b6c-b45a2f598b73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(random_seed)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model = model.to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a419787-8389-41ee-b913-d39f1353a84d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0602c0e3-2dfb-4967-9200-be9899cb1a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766bb098-e2a3-4fbc-b5b0-830519ab16d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, lora_tr_params = report_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f24809-38ae-4bd8-98fd-b0ce2a88c6e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_classes = (ViTEmbeddings, ViTSelfAttention, ViTSelfOutput, ViTIntermediate, ViTOutput)\n",
    "target_classes = (Linear)\n",
    "hook_model(model, report_hook, target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d8fcf0-8f51-4e91-8f37-ac30862272ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_memory()\n",
    "stats = bench_model(model, input_batch, min_run_time=min_run_time)\n",
    "rows.append({**stats, 'lora': 'peft'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc42a0-34d9-4cd1-b4d2-4fe29b271546",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3242011-c241-4bd4-941c-a5412756f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96faf6a-7b32-4912-9bdc-2b33f9f6052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Allocated GPU Memory: {torch.cuda.memory_allocated() / 2**20}MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1283fef-1ce6-44b4-8dde-35998e64ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166639fd-a85f-4d9f-b714-13c38c9ac278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_lora_collection = RunLoRACollection(min_run_time=min_run_time/2)\n",
    "run_lora_mapping = \\\n",
    "    run_lora_collection.optimize_for_model(\n",
    "        model,\n",
    "        n_batch=batch_size,\n",
    "        lora_r=lora_r,\n",
    "        target_modules=target_modules,\n",
    "        criterions=['flops'],\n",
    "        # 224 x 224 -> conv kernel_size=(16, 16), stride=(16, 16)\n",
    "        sequence_length=197)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec28205-cdcb-4d19-9bcf-172ca486342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, run_lora_collection\n",
    "reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f11cb9-7d08-4e1a-bce5-4d38c7630f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "model = model.to(dtype)\n",
    "model = RunLoRAModel(model,\n",
    "                     run_lora_mapping['flops'],\n",
    "                     lora_r=lora_r,\n",
    "                     lora_alpha=lora_alpha,\n",
    "                     lora_dtype=dtype,\n",
    "                     target_modules=target_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b86fc-e806-4851-8224-fff4ff1c0601",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_lora_mapping['flops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0a797-feac-4d4e-8ef4-7054369fc2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every parameter except for lora adapters is set to requires_grad=False\n",
    "model.prepare_for_finetuning(modules_to_save=['classifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7f810-0f29-444b-bd28-bfd5dd9ad730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28c530-b4d8-40a6-8c9a-826b05682e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, runlora_tr_params = report_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2fbc8-4d50-482d-b667-b5fefdcb01cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_classes = (Linear, RunLoRALinear)\n",
    "# target_classes = (ViTEmbeddings, ViTSelfAttention, ViTSelfOutput, ViTIntermediate, ViTOutput)\n",
    "hook_model(model, report_hook, target_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e142b120-4bc9-4c06-89a0-8fbcc39d095d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b94f2-bffe-4a25-9be6-505496b0e398",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert lora_tr_params == runlora_tr_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495f873a-472a-4a4a-88c0-9d0a4faad466",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_memory()\n",
    "stats = bench_model(model, input_batch, min_run_time=min_run_time)\n",
    "rows.append({**stats, 'lora': 'runlora'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b2da7-cc2b-4caa-8d5e-6aea1a58702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "logging.info(f'Max GPU Memory Reserved: {torch.cuda.max_memory_reserved() / 2**20} MB')\n",
    "reset_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2aac78-13f2-4da4-af8e-9d69c15c85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(rows)\n",
    "df.sort_values(['mean_time_us', 'max_mem_overhead_MB'],\n",
    "               ascending=[True, True], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51a6a6-85b8-4464-9353-592ee1a613d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - df.mean_time_us.iloc[0] / df.mean_time_us.iloc[1], 1 - df.mean_time_us.iloc[0] / df.mean_time_us.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f2a21f-588c-4d4c-9982-e780f972cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split('/')[-1].split('-patch')[0]\n",
    "df.to_csv(f'{model_name}_r{lora_r}b{batch_size}_{type_string}'+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e22cda-fc12-45bb-a32c-9a5b4a29e8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
